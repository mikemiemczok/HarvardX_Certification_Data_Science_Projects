---
title: "Movie rating prediction - Report"
author: "Mike Miemczok"
date: "18 4 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")

library(lubridate)
library(tidyverse)
library(caret)
library(data.table)
library(knitr)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")


# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```


## Introduction

The Dataset used in this analysis is the MovieLens 10M dataset provided by grouplens.org.
It contains two different datasets - one (movies.dat) containing the userID, the movieID, rating, timestamp and the other one (ratings.dat) containing the movieID, the title of the movie and the genres. The movieID column on the ratings data frame is then joined with the movieID column of the movie data frame with the corresponding information. 

Afterwards a test and training set is created using the createDataPartition method dividing the data randomly in a 90 - 10 split. The training set named edx contains now 90% of the data. The remaining 10% are stored in validation, the test set which will be used to validate the training method later on.

The structure of the training and test set is as followed:


```{r, echo=FALSE}
str(edx)
```

The raw dataset contains ratings by different users for many movies in different genres over a large timespan:

```{r, echo = FALSE}

head(edx)

```

The goal of the project is to predict ratings, given by the information of the MovieLens dataset.

Following keysteps were made:

1. Exploring the dataset.
2. Preprocessing the dataset.
3. Visualising the data.
4. Choosing linear regression as the main model.
5. Train the model and choose the right predictors using correlation.
6. Choose the right model to predict the ratings.
7. Present the results.

\newpage

## Analysis

### Data preprocessing

The raw dataset has shown that the timestamp is not really readable.

As first step in the data preprocessing it is necessary to transform the timestamp in to a readable date for both datasets (timeformat).
Afterwards this information is stripped down to month and year in another column named monthyear.

```{r, echo=FALSE}

########### Add Column timeformat based on TIMESTAMP into an interpretable time format (yyyy-mm-dd hh:mm:ss UTC). #############

edx <- edx %>% mutate(timeformat = as_datetime(timestamp))
validation <- validation %>% mutate(timeformat = as_datetime(timestamp))
edx %>% select(timestamp, timeformat) %>% head(3)

######### Add column monthyear based on timeformat which contains the year and month (yyyy-mm). ########

edx <- edx %>% mutate(monthyear = paste(month(timeformat),year(timeformat),sep = "-"))
validation <- validation %>% mutate(monthyear = paste(month(timeformat),year(timeformat),sep = "-"))
edx %>% select(timestamp, timeformat, monthyear) %>% head(3)



```

The genre column is not atomic. That means that a movie can have more genres. It is usefull to seperate the genre in more records.

```{r, echo=FALSE}
edx %>% filter(movieId == 122 & userId == 1 ) %>% select(title, genres)
edx <- edx %>% separate_rows(genres, sep = "\\|")
validation <- validation %>% separate_rows(genres, sep = "\\|")

```

After the seperation the genre looks like:

```{r, echo=FALSE}

edx %>% filter(movieId == 122 & userId == 1 ) %>% select(title, genres)

```

\newpage

### Visualization

To gain a better insight into the dataset, the first step was to look a little closer at the information given in the dataset.

```{r, echo= FALSE}
edx %>%
  group_by(rating) %>%
  summarise(count = n()) %>%
  ggplot(aes(rating, count/1000000)) +
  geom_col() +
  labs(title = "General distribution of the ratings",
       x = "Rating",
       y = "Prevelance in millions")
```

As we can see from the plot shown above the most given rating is 4, followed by 3 and 5. Half star ratings seem to be less likely than whole star ratings. The mean rating seems to be above the mean of the scale 2.5 around 3.5.

\newpage

The most rated movies are:

```{r, echo=FALSE}
edx %>%
  group_by(title) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10) %>%
  ggplot(aes(reorder(title, -count), count)) +
  geom_col() +
  scale_x_discrete(guide = ggplot2::guide_axis(n.dodge = 2), 
                   labels = function(x) stringr::str_wrap(x, width = 20)) +
  labs(title = "Ten most rated movies",
       x = "Movie",
       y = "Prevelance")
```

As we can see the most rated movies are all from the 90s. This is most likely due to the fact that movielens ratings start from 1995, so movies from the 90s had a lot more time to accumulate ratings than recent movies.

\newpage

The less rated movies with only one rating each are:

```{r, echo=FALSE}
edx %>%
  group_by(title) %>%
  summarise(count = n()) %>%
  arrange(count) %>%
  select(title) %>%
  head(10)
```

There are over ten thousand movies with only one rating. Due to this occurrence these film tend to be rated more extreme - more positive or negative - compared to movies with thousands of ratings. This effect will be taken into account during regularization with the usage of lambda.

\newpage

With the next two visualizations the goal is to show if there is a correlation between the rating and the time of the rating.

```{r, echo=FALSE}
edx %>%
  mutate(month = month(timeformat)) %>%
  group_by(month) %>%
  summarise(rating_avg = mean(rating)) %>%
  ggplot(aes(month, rating_avg)) +
  geom_point() +
  labs(title = "General distribution of the ratings",
       x = "Month in Numbers",
       y = "Avg. Rating")

```
```{r, echo=FALSE}
edx %>%
  mutate(month = month(timeformat)) %>%
  group_by(month) %>%
  summarise(count = n()) %>%
  ggplot(aes(month, count/1000000)) +
  geom_point() +
  labs(title = "Prevelance of ratings in diffrent months",
       x = "Month in Numbers",
       y = "Prevelance of Ratings in millions")
```

We will look at the correlation in detail later, but we can already see there is most likely very little to no correlation between the time of the rating and the rating itself.

\newpage

### Building the model

##### Model: Mean(Rating)
\hfill\break
Next up the edx set is divided into a training set and a test set using a 80 - 20 split (80% train set, 20% test set).
To test the model later without problems all the entries in the test set, that are not in the training set, are removed and added to the training set.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
temp_test_set <- edx[test_index,]

# Make sure userId and movieId in test set are also in training set
test_set <- temp_test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Add rows removed from test set back into training set
removed <- anti_join(temp_test_set, test_set)
train_set <- rbind(train_set, removed)
```

The naive approach would be to guess the rating based on the mean rating over all movies. 
The following formula represents this approach, where $Y_{i, u, g, my}$ represents the predicted rating, $\mu$ the mean rating and $\epsilon_{i, u, g, my}$ the independent errors:


$$Y_{i, u, g, my} = \mu + \epsilon_{i, u, g, my}$$

The mean rating for all movies for the train set is:

```{r, echo=FALSE}
mu <- mean(train_set$rating)
mu
```

Based on this model we can check with the test set how well the model is currently performing.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# predict the ratings given only the mean(rating).
predicted_ratings <- test_set %>%
  mutate(pred = mu)

# calculate and display the RMSE for mean(rating).
model_0_rmse <- sqrt(mean((test_set$rating - predicted_ratings$pred)^2))
rmse_results <- data_frame(method = "Mean(Rating)", RMSE = model_0_rmse)
rmse_results %>% knitr::kable()
```

Using the naive model using only the mean(rating) the obtained RMSE using the test set is: 1.052367

\newpage

#### Model: Mean(Rating) + Movie
\hfill\break
To gain an insight on the relationship between the predictors and the final rating we first look at the correlation between each predictor and the final rating.

```{r, echo=FALSE}
# Calculate the influence of movie to the rating.
movie_vals <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i_ind = mean(rating - mu))

# Calculate the influence of user to the rating.
user_vals <- train_set %>%
  group_by(userId) %>% 
  summarize(b_u_ind = mean(rating - mu))

# Calculate the influence of genre to the rating.
genre_vals <- train_set %>%
  group_by(genres) %>% 
  summarize(b_g_ind = mean(rating - mu))

# Calculate the influence of monthyear to the rating.
monthyear_vals <- train_set %>%
  group_by(monthyear) %>% 
  summarize(b_my_ind = mean(rating - mu))

#Correlation between rating and the movie predictor
cor_rating_movie_ind <- test_set %>% 
  left_join(movie_vals, by='movieId') %>%
  summarize(r = cor(rating, b_i_ind))

cor_results <- data_frame(Variables = "Rating ~ Movie (Independent)", Correlation = cor_rating_movie_ind$r)

#Correlation between rating and the user predictor
cor_rating_user_ind <- test_set %>% 
  left_join(user_vals, by='userId') %>%
  summarize(r = cor(rating, b_u_ind))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Rating ~ User (Independent)", Correlation = cor_rating_user_ind$r))

#Correlation between rating and the genre predictor
cor_rating_genre_ind <- test_set %>% 
  left_join(genre_vals, by='genres') %>%
  summarize(r = cor(rating, b_g_ind))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Rating ~ Genre (Independent)", Correlation = cor_rating_genre_ind$r))

#Correlation between rating and the monthyear predictor
cor_rating_monthyear_ind <- test_set %>% 
  left_join(monthyear_vals, by='monthyear') %>%
  summarize(r = cor(rating, b_my_ind))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Rating ~ Monthyear (Independent)", Correlation = cor_rating_monthyear_ind$r))
cor_results %>% knitr::kable()

```

As we can see in the table movie has the strongest correlation and therefore impact on the rating. 
The second strongest predictor is User followed by genre and monthyear.
Therefore we choose the movie predictor as the first variable used in the linear regression model.

The approach is to guess the rating based on the mean rating over all movies and the movie effect.
The movie effect is the influence the movie has on the rating, obtained by the mean rating of this movie in relation to the mean(rating).
The following formula represents this approach, where $Y_{i, u, g, my}$ represents the predicted rating, $\mu$ the mean rating, $b_{i}$ the movie effect and $\epsilon_{i, u, g, my}$ the independent errors:


$$Y_{i, u, g, my} = \mu + b_{i} + \epsilon_{i, u, g, my}$$

Here is an example of 5 values that show the effect the movie has on the rating and the mean influence the movie effect has on the final rating.
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Calculate the influence of movies to the rating.
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

# Calculate the influence the predictor has on the model.
model_1_predictor <- sqrt(mean((movie_avgs$b_i)^2))

movie_avgs %>%
  head(5)

model_1_predictor
```

Based on this model we can check with the test set how well the adjusted model is currently performing.

```{r, echo=FALSE}
# predict the ratings given the movie effect.
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(pred = mu + b_i)

# calculate and display the RMSE for mean(rating) + movie.
model_1_rmse <- sqrt(mean((test_set$rating - predicted_ratings$pred)^2))
rmse_results <- bind_rows(rmse_results, data_frame(method="Movie",  RMSE = model_1_rmse, Predictorweight = model_1_predictor))
rmse_results %>% knitr::kable()
```

Using the adjusted model using only the mean(rating) and the movie effect the obtained RMSE using the test set is: 0.9411548

\newpage

#### Model: Mean(Rating) + Movie + User
\hfill\break
Based on the independent correlation between user and rating we choose user as the next variable for the regression model.
But first we need to make sure movie and user do not correlate much, as user would not improve our model much then.

```{r, echo=FALSE}
#Correlation between movie predictor and the user predictor
cor_movie_user_ind <- test_set %>% 
  left_join(movie_vals, by='movieId') %>%
  left_join(user_vals, by='userId') %>%
  summarize(r = cor(b_i_ind, b_u_ind))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Movie ~ User (Independent)", Correlation = cor_movie_user_ind$r))
cor_results %>% knitr::kable()
```

As we can see in the table movie and user do not correlate much, so we include user as the next variable in our model.

The approach is to guess the rating based on the mean rating over all movies and the movie effect + the user effect.
The user effect is the influence the user has on the rating, obtained by the mean rating of this user.
The following formula represents this approach, where $Y_{i, u, g, my}$ represents the predicted rating, $\mu$ the mean rating, $b_{i}$ the movie effect, $b_{u}$ the user effect and $\epsilon_{i, u, g, my}$ the independent errors:


$$Y_{i, u, g, my} = \mu + b_{i} + b_{u} + \epsilon_{i, u, g, my}$$

Here is an example of 5 values that show the effect the user has on the rating and the mean influence the user effect has on the final rating.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Calculate the influence of users to the rating given the movie effect.
user_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>% 
  summarize(b_u = mean(rating - mu - b_i))

# Calculate the influence the predictor has on the model.
model_2_predictor <- sqrt(mean((user_avgs$b_u)^2))

user_avgs %>%
  head(5)

model_2_predictor
```

Based on this model we can check with the test set how well the adjusted model is currently performing.

```{r, echo=FALSE}
# predict the ratings given the movie + user effect.
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u)

# calculate and display the RMSE for mean(rating) + movie + user.
model_2_rmse <- sqrt(mean((test_set$rating - predicted_ratings$pred)^2))
rmse_results <- bind_rows(rmse_results, data_frame(method="Movie + User",  RMSE = model_2_rmse, Predictorweight = model_2_predictor))
rmse_results %>% knitr::kable()
```

Using the adjusted model using only the mean(rating) and the movie effect + user effect the obtained RMSE using the test set is: 0.8578850

```{r, echo=FALSE}
#Correlation between rating and the user predictor depending on the movie predictor
cor_movie_user <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  summarize(r = cor(rating, b_u))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Rating ~ User (Movie)", Correlation = cor_movie_user$r))
cor_results %>% knitr::kable()
```

As we can see in the correlation table the adjusted user variable (dependent on movie) still correlates with the rating even after the information of the movie is subtracted from it. Therefore there is still a high information gain from the user variable and we can use it to predict the rating more accurately.

\newpage

#### Model: Mean (Rating) + Movie + User + Genre
\hfill\break
Based on the independent correlation between genre and rating we choose genre as the next variable for the regression model.
But first we need to make sure movie and genre or user and genre do not correlate much, as genre would not improve our model much then.

```{r, echo=FALSE}
#Correlation between movie predictor and the genre predictor
cor_movie_genre_ind <- test_set %>% 
  left_join(movie_vals, by='movieId') %>%
  left_join(genre_vals, by='genres') %>%
  summarize(r = cor(b_i_ind, b_g_ind))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Movie ~ Genre (Independent)", Correlation = cor_movie_genre_ind$r))

#Correlation between user predictor and the genre predictor
cor_user_genre_ind <- test_set %>% 
  left_join(user_vals, by='userId') %>%
  left_join(genre_vals, by='genres') %>%
  summarize(r = cor(b_u_ind, b_g_ind))

cor_results <- bind_rows(cor_results, data_frame(Variables = "User ~ Genre (Independent)", Correlation = cor_user_genre_ind$r))
cor_results %>% knitr::kable()
```

As we can see in the table user and genre do not correlate much, but movie and genre do. This makes sense as a movie passively already includes the information of the genres it belongs to. We still include the genre variable in our model to see if there would be an improvement, but expect to see not much improvement due to the correlation with movie.

The approach is to guess the rating based on the mean rating over all movies and the movie effect + the user effect + the genre effect.
The genre effect is the influence the genre has on the rating, obtained by the mean rating of this genre.
The following formula represents this approach, where $Y_{i, u, g, my}$ represents the predicted rating, $\mu$ the mean rating, $b_{i}$ the movie effect, $b_{u}$ the user effect, $b_{g}$ the genre effect and $\epsilon_{i, u, g, my}$ the independent errors:


$$Y_{i, u, g, my} = \mu + b_{i} + b_{u} + b_{g} + \epsilon_{i, u, g, my}$$

Here is an example of 5 values that show the effect the genre has on the rating and the mean influence the genre effect has on the final rating.
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Calculate the influence of the genre to the rating given the movie + user effect.
genre_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>% 
  summarize(b_g = mean(rating - mu - b_i - b_u))

# Calculate the influence the predictor has on the model
model_3_predictor <- sqrt(mean((genre_avgs$b_g)^2))

genre_avgs %>%
  head(5)

model_3_predictor
```

Based on this model we can check with the test set how well the adjusted model is currently performing.

```{r, echo=FALSE}
# predict the ratings given the movie + user + genre effect.
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  mutate(pred = mu + b_i + b_u + b_g)

# calculate and display the RMSE for mean(rating) + movie + user + genre.
model_3_rmse <- sqrt(mean((test_set$rating - predicted_ratings$pred)^2))
rmse_results <- bind_rows(rmse_results, data_frame(method="Movie + User + Genres",  RMSE = model_3_rmse, Predictorweight = model_3_predictor ))
rmse_results %>% knitr::kable()
```

Using the adjusted model using only the mean(rating) and the movie effect + user effect + genre effect the obtained RMSE using the test set is: 0.8578009

```{r, echo=FALSE}
#Correlation between rating and the genre predictor depending on the movie + user predictor
cor_user_genre <- test_set %>% 
  left_join(genre_avgs, by='genres') %>%
  summarize(r = cor(rating, b_g))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Rating ~ Genre (Movie + User)", Correlation = cor_user_genre$r))
cor_results %>% knitr::kable()
```

As we can see in the correlation table the adjusted genre variable (dependent on movie and user) does not correlate with the rating even after the information of the movie and user is subtracted from it. Therefore genre does not provide enough new information to our model to make it worth including. We do not continue to use the genre predictor in our prediction model.

\newpage

#### Model: Mean (Rating) + Movie + User + Monthyear
\hfill\break
Based on the independent correlation between monthyear and rating we choose monthyear as the next variable for the regression model.
But first we need to make sure movie and monthyear or user and monthyear do not correlate much, as monthyear would not improve our model much then.
Also monthyear has a low correlation with rating to begin with, so it might not be worth including it in the first place.

```{r, echo=FALSE}
#Correlation between movie predictor and the monthyear predictor
cor_movie_monthyear_ind <- test_set %>% 
  left_join(movie_vals, by='movieId') %>%
  left_join(monthyear_vals, by='monthyear') %>%
  summarize(r = cor(b_i_ind, b_my_ind))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Movie ~ Monthyear (Independent)", Correlation = cor_movie_monthyear_ind$r))

#Correlation between user predictor and the monthyear predictor
cor_user_monthyear_ind <- test_set %>% 
  left_join(user_vals, by='userId') %>%
  left_join(monthyear_vals, by='monthyear') %>%
  summarize(r = cor(b_u_ind, b_my_ind))

cor_results <- bind_rows(cor_results, data_frame(Variables = "User ~ Monthyear (Independent)", Correlation = cor_user_monthyear_ind$r))
cor_results %>% knitr::kable()
```

As we can see in the table movie and monthyear do not correlate much and user and monthyear only correlate a little bit. We include the monthyear variable in our model to see if there would be an improvement, but expect to see not much improvement due to the low correlation with the rating to begin with.

The approach is to guess the rating based on the mean rating over all movies and the movie effect + the user effect + the monthyear effect.
The monthyear effect is the influence the timing - meaning the month and year of the rating - has on the rating, obtained by the mean rating for a specific month and year.
The following formula represents this approach, where $Y_{i, u, g, my}$ represents the predicted rating, $\mu$ the mean rating, $b_{i}$ the movie effect, $b_{u}$ the user effect, $b_{my}$ the monthyear effect and $\epsilon_{i, u, g, my}$ the independent errors:


$$Y_{i, u, g, my} = \mu + b_{i} + b_{u} + b_{my} + \epsilon_{i, u, g, my}$$

Here is an example of 5 values that show the effect the timing - month and year - has on the rating and the mean influence the monthyear effect has on the final rating.
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Calculate the influence of the monthyear to the rating given the movie + user + genre effect
monthyear_avgs <- train_set %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(monthyear) %>% 
  summarize(b_my = mean(rating - mu - b_i - b_u))

# Calculate the influence the predictor has on the model
model_4_predictor <- sqrt(mean((monthyear_avgs$b_my)^2))

monthyear_avgs %>%
  head(5)

model_4_predictor
```

Based on this model we can check with the test set how well the adjusted model is currently performing.

```{r, echo=FALSE}
# predict the ratings given the movie + user + monthyear effect.
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(monthyear_avgs, by='monthyear') %>%
  mutate(pred = mu + b_i + b_u + b_my)

# calculate and display the RMSE for mean(rating) + movie + user + monthyear.
model_4_rmse <- sqrt(mean((test_set$rating - predicted_ratings$pred)^2))
rmse_results <- bind_rows(rmse_results, data_frame(method="Movie + User + MonthYear",  RMSE = model_4_rmse, Predictorweight = model_4_predictor ))
rmse_results %>% knitr::kable()
```

Using the adjusted model using only the mean(rating) and the movie effect + user effect + genre effect + monthyear effect the obtained RMSE using the test set is: 0.8578550

```{r, echo=FALSE}
#Correlation between rating and the monthyear predictor depending on the movie + user predictor
cor_user_monthyear <- test_set %>% 
  left_join(monthyear_avgs, by='monthyear') %>%
  summarize(r = cor(rating, b_my))

cor_results <- bind_rows(cor_results, data_frame(Variables = "Rating ~ Monthyear (Movie + User)", Correlation = cor_user_monthyear$r))
cor_results %>% knitr::kable()
```

As we can see in the correlation table the adjusted monthyear variable (dependent on movie and user) still does not correlate with the rating. Therefore genre does not provide enough new information to our model to make it worth including. We do not continue to use the monthyear predictor in our prediction model.

\newpage

#### Regularization
\hfill\break
Regularization is a technique that can be used to optimize the weight of the predictors. In the example of ratings for movies it needs a minimum ammount of ratings that the calculated avg. rating is meaningful. 
To regularize the parameters such as $b_{i}$ the movie effect and $b_{u}$ the user effect we are going to calculate lambda for each of them.

Crossvalidation is used to get the optimal lambda for $b_{i}$ the movie effect.
The effect the lambda has on the RMSE is displayed in the following plot:

```{r, echo=FALSE}
# Using crossvalidation to get the optimal lambda for the movie effect.
lambdas <- seq(0, 10, 0.5)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    mutate(pred = mu + b_i)
  return(sqrt(mean((test_set$rating - predicted_ratings$pred)^2)))
})

# plot the different lambdas against the RMSEs to select lambda with the lowest RMSE value.
qplot(lambdas, rmses)  

```

As we see the optimal lamda for $b_{i}$ the movie effect is: 

```{r, echo=FALSE}
 # Get the lambda with the lowest RMSE.
lambda_movie <- lambdas[which.min(rmses)]
lambda_movie
```

\newpage

Crossvalidation is used to get the optimal lambda for $b_{u}$ the user effect.
The effect the lambda has on the RMSE is displayed in the following plot:

```{r, echo=FALSE}
# Using crossvalidation to get the optimal lambda for the user effect.
lambdas <- seq(0, 10, 0.5)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda_movie))
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u)
  return(sqrt(mean((test_set$rating - predicted_ratings$pred)^2)))
})

# plot the different lambdas against the RMSEs to select lambda with the lowest RMSE value.
qplot(lambdas, rmses)
```

As we see the optimal lamda for $b_{u}$ the user effect is: 

```{r, echo=FALSE}
# Get the lambda with the lowest RMSE.
lambda_user <- lambdas[which.min(rmses)]
lambda_user
```

Once the lambdas are set for each of the biased effects such as $b_{i}$ the movie effect, $b_{u}$ the user effect, $b_{g}$ the genre effect and $b_{my}$ the monthyear effect we can predict the ratings using generalized effect values.

We are going to apply each of the lambdas step by step.

\newpage

First we are going to use only the generalized $b_{i}(\lambda)$ movie effect:

$$Y_{i, u, g, my} = \mu + b_{i}(\lambda) + \epsilon_{i, u, g, my}$$

```{r, echo=FALSE}
#Calculating the movie effect using the corresponding lambda
b_i <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda_movie))

# Calculate the influence the predictor has on the model
model_5_predictor <- sqrt(mean((b_i$b_i)^2))

# predict the ratings given the generalized movie effect.
predicted_ratings <- 
  test_set %>% 
  left_join(b_i, by = "movieId") %>%
  mutate(pred = mu + b_i)

# calculate and display the RMSE for mean(rating) + generalized movie.
model_5_rmse <- sqrt(mean((test_set$rating - predicted_ratings$pred)^2))
rmse_results <- bind_rows(rmse_results, data_frame(method="Generalized Movie",  RMSE = model_5_rmse, Predictorweight = model_5_predictor ))
rmse_results %>% knitr::kable()
```

Then we are going to add $b_{u}(\lambda)$ the movie effect:

$$Y_{i, u, g, my} = \mu + b_{i}(\lambda) + b_{u}(\lambda) + \epsilon_{i, u, g, my}$$

```{r, echo=FALSE}
#Calculating the user effect using the corresponding lambda of user.
b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_user))

# Calculate the influence the predictor has on the model
model_6_predictor <- sqrt(mean((b_u$b_u)^2))

# predict the ratings given the generalized movie and generalized user effect.
predicted_ratings <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u)

# calculate and display the RMSE for mean(rating) + generalized movie and generalized user.
model_6_rmse <- sqrt(mean((test_set$rating - predicted_ratings$pred)^2))
rmse_results <- bind_rows(rmse_results, data_frame(method="Generalized Movie + User",  RMSE = model_6_rmse, Predictorweight = model_6_predictor ))
rmse_results %>% knitr::kable()
```

\newpage

## Results


The final rating will be using $b_{i}(\lambda)$ the generalized movie effect and $b_{u}(\lambda)$ the generalized user effect.

Final Model:

$$Y_{i, u, g, my} = \mu + b_{i}(\lambda) + b_{u}(\lambda) + \epsilon_{i, u, g, my}$$

When training the final model on the edx data set we get a RMSE of 0.8630447 using the validation set to calculate the RMSE.

```{r, echo=FALSE}
#Final Model: Generalized Movie + User. Genres + MonthYear are irrelevant also seen in lambda -> infinity

#training the model with the mean of the training set edx
mu <- mean(edx$rating)

#including the movie-effect based on the training set edx and the corresponding lambda
b_i <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda_movie))

#including the user-effect based on the training set edx and the corresponding lambda
b_u <- edx %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_user))

#predicting the ratings based on the generalized model using the predictors movie and user
predicted_ratings <- 
  validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u)
model_7_rmse <- sqrt(mean((validation$rating - predicted_ratings$pred)^2))
rmse_results <- data_frame(method = "Generalized Movie + User", RMSE = model_7_rmse)
rmse_results %>% knitr::kable()

```


## Conclusion

Based on different movie ratings from different years the model can predict ratings for upcoming movies with a RMSE of 0.8630447. That means that true prediction value is on average in a range between -0.8630447 and 0.8630447. One of the limitations in this dataset is that there aren't enough attributes that can be used for more accurate predictions. Either the other attributes do not give us more information (monthyear) or the attribute correlates with an attribute already used in the prediction (genre ~ movie), so including does not lead to a more accurate prediction.

Feel free to network with me on LinkedIn: https://www.linkedin.com/in/mike-miemczok-432206209/ 

or Xing: https://www.xing.com/profile/Mike_Miemczok

Also check out my github for future work: https://github.com/mikemiemczok